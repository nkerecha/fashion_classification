{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Fashion minst dataset that is available in the keras data library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain it through the keras dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the load data will give two sets of lists: one will be used to train and the other used to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(training_images, training_labels),(testing_images, testing_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view an example of what a training example and a training label look like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(linewidth = 200)\n",
    "plt.imshow(training_images[0]) # Displays the image at the index 0\n",
    "print(training_labels[0]) # Prints the value assigned at the index 0 \n",
    "print(training_images[0]) # Prints the array representing the images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All vallues currently lie between 0 - 255. It would make the model more efficient to work with if our values lied between 0 - 1. A process called normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_images / 255.0\n",
    "testing_images = testing_images / 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the model \n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128,activation = tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation = tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequential - Defines a sequence of layers \n",
    "- Flatten - Converts the 2d array to a 1d array \n",
    "- Dense - Adds a Layer of neurons. Each layer of neurons needs an activation function to tell them what to do.\n",
    "- Relu - Means  that \"If X > 0 return X\" else return 0 -- This results in only values greater than zero being passed to the next layer of the neural network\n",
    "- Softmax- Takes a set/array of values and returns the greatest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.4973 - accuracy: 0.8252\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.3715 - accuracy: 0.8661\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.3338 - accuracy: 0.8784\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.3109 - accuracy: 0.8863\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2919 - accuracy: 0.8924\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2795 - accuracy: 0.8970\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.2684 - accuracy: 0.9006\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2557 - accuracy: 0.9058\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2464 - accuracy: 0.9095\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2388 - accuracy: 0.9117\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 4s 58us/sample - loss: 0.2287 - accuracy: 0.9150\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.2228 - accuracy: 0.9164\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2169 - accuracy: 0.9190\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.2094 - accuracy: 0.9221\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2050 - accuracy: 0.9240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x263e2c24588>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model \n",
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.3562 - accuracy: 0.8843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3562468178987503, 0.8843]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model \n",
    "model.evaluate(testing_images,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE THE RESULTS OF MAKING A FEW CHANGES AND PARAMETERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4983100e-07 1.5559297e-10 3.1672974e-07 5.2494994e-16 2.1393107e-07 8.8499663e-03 1.5684606e-08 6.2038349e-03 3.2921193e-06 9.8494220e-01]\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(testing_images)\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the probability that the image at the index 0 is classified under one of the values at 0 - 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impact of having more neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.1856\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0740\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.0483\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 153us/sample - loss: 0.0339\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0278\n",
      "10000/10000 [==============================] - 1s 109us/sample - loss: 0.0697\n",
      "[2.5721378e-09 6.8235256e-10 3.7861634e-09 1.5230566e-05 1.3986398e-12 9.8488284e-09 2.0070634e-14 9.9998450e-01 7.6159390e-10 2.6174564e-07]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Experiment with a model that has more neurons \n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having more neurons in your neural net makes the training take longer but results in better model accuracy \n",
    "- More neurons being presents results in more calculations slowing down the model training process but as noted above the accuracy of the model does improve.\n",
    "- This does not mean that the more neurons you have in your netwrk the better,  you canhit the law of diminishing return very quickly [ Might be infering to model overfitting]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We know that we have ten classifications for the fashion. What would haven if we did not have 10 layers and iinstead put a smaller number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of neurons should match the number of classes that we would like to map our data to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impact of Using an additional layer of neurons**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What would be the impact of additional layers on the neural network\n",
    "- In this case we will add one additional layer with 512 neurons and have the final neuron as expected with the ten neurons that represent the different classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.1840\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0785\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.0552\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0409\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0319\n",
      "10000/10000 [==============================] - 1s 80us/sample - loss: 0.0767\n",
      "[6.4146947e-12 1.1197429e-07 4.9128605e-08 2.3618780e-07 6.8813144e-10 3.1014424e-11 3.1402252e-13 9.9999118e-01 9.8889778e-11 8.4211579e-06]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The impact of additional layers of neurons is not significant since we do not have data that is very complex.\n",
    "- In other cases where the data is very complex, it may be necessary to have additional layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impact of Using more or less epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.2612\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1131\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0772\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0584\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0447\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.0360\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0273\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0239\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0186\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0152\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0127\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0123\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 4s 58us/sample - loss: 0.0101\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0085\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0072\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.007 - 4s 59us/sample - loss: 0.0079\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0058\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0077\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0044\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0044\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0071\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0030\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0051\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0042\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0049\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0043\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0049\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0023\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0056\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0037\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.1160\n",
      "[1.5685882e-23 9.0361136e-15 3.1222251e-15 7.8982446e-12 1.7318874e-26 7.4591956e-33 3.1322834e-22 1.0000000e+00 2.4393616e-20 2.1907002e-16]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=30)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[34])\n",
    "print(test_labels[34])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model get a much better loss value with more epochs. At some point however the loss value stopd decreasing and begins increasing which is a side effect of overfiting.\n",
    "- There is no point in training the neural network if you aren't improving your loss value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impact of not normalizing the data before training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 2.5155\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.3500\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2785\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2607\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2368\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2197\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2018\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2007\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.1882\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1850\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.1808\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1732\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1687\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.1706\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1690\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.1641\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1653\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.1576\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.156 - 3s 56us/sample - loss: 0.1560\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.1503\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.1503\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1521\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.1481\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1486\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.1409\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1469\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.1404\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.1386\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.1426\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.1328\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.4562\n",
      "[0.0000000e+00 3.6278424e-17 3.2396698e-13 2.5129451e-11 2.6432120e-33 5.4827498e-30 0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5333245e-35]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=30)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[34])\n",
    "print(test_labels[34])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Failing to normalize the data results in very massive loss values despite the fact that the model prediction is accurate.\n",
    "- Therefore normalizing the data results in better loss and thus better model efficiency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a call back to stop epochs once a desired accuracy is achieved**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To do tthis we use call backs to stop the training once a desired accuracy is reached \n",
    "- This can also be used to prevent overfitting which can be noted when the loss values that were decreasing begin to increase and then continue fluctuating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4747 - accuracy: 0.8296\n",
      "Epoch 2/5\n",
      "59872/60000 [============================>.] - ETA: 0s - loss: 0.3574 - accuracy: 0.8694\n",
      "Reached Loss < 0.4. Training has been cancelled!\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.3573 - accuracy: 0.8694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26380047508>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)\n",
    "\n",
    "# create a call back class that has desired loss parameters \n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        if (logs.get('loss') < 0.4):\n",
    "            print(\"\\nReached Loss < 0.4. Training has been cancelled!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "# Initialize an instance of the class myCallback()\n",
    "callbacks = myCallback()\n",
    "\n",
    "# Declare/Initialize the fashion dataset and load the data\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images,training_labels),(testing_images,testing_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the data( training images and the testing images) \n",
    "training_images = training_images / 255.0\n",
    "testing_images = testing_images / 255.0\n",
    "\n",
    "# Initialize the layers of the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                   tf.keras.layers.Dense(512, activation = tf.nn.relu),\n",
    "                                   tf.keras.layers.Dense(10, activation = tf.nn.softmax)])\n",
    "\n",
    "# Train the mode;;\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs = 5,callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "59872/60000 [============================>.] - ETA: 0s - loss: 0.4799 - accuracy: 0.8285\n",
      "Reached 80% accuracy. Training has been cancelled!\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.4796 - accuracy: 0.8285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2638015a3c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)\n",
    "\n",
    "# create a call back class that has desired loss parameters \n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        if (logs.get('accuracy') > 0.8):\n",
    "            print(\"\\nReached 80% accuracy. Training has been cancelled!\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "# Initialize an instance of the class myCallback()\n",
    "callbacks = myCallback()\n",
    "\n",
    "# Declare/Initialize the fashion dataset and load the data\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images,training_labels),(testing_images,testing_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the data( training images and the testing images) \n",
    "training_images = training_images / 255.0\n",
    "testing_images = testing_images / 255.0\n",
    "\n",
    "# Initialize the layers of the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                   tf.keras.layers.Dense(512, activation = tf.nn.relu),\n",
    "                                   tf.keras.layers.Dense(10, activation = tf.nn.softmax)])\n",
    "\n",
    "# Train the mode;;\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs = 5,callbacks = [callbacks])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
